\documentclass{article}
\usepackage[a4paper]{geometry}
\usepackage[utf8]{inputenc} 
\usepackage{indentfirst}
%list code
\usepackage{listings}
%content include references
\usepackage[nottoc,numbib]{tocbibind}
% table of contents clickable
\usepackage{hyperref}
% make file system neat and easier to manage
\usepackage{subfiles}
% packages
\usepackage{csquotes}
\usepackage{arydshln}
\usepackage{dsfont}
\usepackage{polynom}
\usepackage{empheq}
\usepackage{calc}  
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{systeme}
\usepackage{mathtools}
\usepackage{tikz}
\usepackage{amsfonts, mathrsfs, bm, amsmath, amssymb, bbm, amsthm}
\usepackage{ifthen}
\usepackage{enumerate}
% Make the preview part more eye friendly
\usepackage{xcolor}
\pagecolor[rgb]{1,1,1} % background
\color[rgb]{0,0,0} % foreground
% new command
\DeclareMathOperator*{\argmax}{arg~max}
\DeclareMathOperator*{\argmin}{arg~min}
% theorem block
\newtheorem{theorem}{Theorem}[section]
\newtheorem{note}{Note}
% cross page equation
\allowdisplaybreaks

% TODO: Add references latter
% http://users.stat.umn.edu/~helwig/notes/RandomVariables.pdf

\begin{document}

    \section{First and First}

        \textit{Random} itself has the meaning like \textit{unknown}, and
        \textit{unpredictable}. It'll be a contradiction if we can define
        what is \textit{randomness}.

    \section{Random Variable}

        A \textit{random variable} is a function that maps the outcome of a
        random experiment to a real number. We use $ \mathcal{C} $ to
        denote the set of all outcomes; $ c $ to denote a single outcome.

        For example, let $ C =  \{ \text{GPT}, \text{GAN}, \text{BERT},
        \text{YOLO} \} $, then a random varialbe $ X $ can be
        \begin{equation*}
             X(\text{GPT}) = 0, X(\text{GAN}) = 1, X(\text{BERT}) = 2,
             X(\text{YOLO}) = 3.
        \end{equation*}

        You can change to any number you want.

        We cannot observe a random variable itself, i.e., the mapping $ X $
        is unobservable. We can only define the mapping, and then observe
        the result of applying this mapping to an experiment outcome.

        The \textit{realization} of a random variable is the result of
        applying the random variable (i.e., mapping) to an observed outcome
        of a random experiment. This is what we actually observe.

        Typically, we use lowercase to denote the realized number;
        uppercase to denote the random variable. e.g., $ x $ is a
        realization of $ X $.

        \subsection{Probability Mass Function}

            A random variable $ X $ is said to be \textit{descrete} if its
            space $ \mathcal{D} $ is either finite or countable.

            Let $ X $ be a discrete random variable with space $ \mathcal{D} $.
            The \textit{probability mass function} of $ X $, $ p_{X}(d_{i}) $,
            is defined by

            \begin{equation*}
                p_{X}(d_{i}) = P[\{ c: X(c) = d_{i} \}] = P[ X = d_{i} ],
            \end{equation*}

            for all $ d_{i} \in \mathcal{D} $.

            The induced probability distribution, $ P_{X}(\cdot) $, of $ X $ is

            \begin{equation*}
                P_{X}(D) = \sum_{ d_{i} \in D} p_{X}(d_{i}) = \sum_{ d_{i} \in
                D} P[\{ c: X(c) = d_{i} \}] = \sum_{d_{i} \in D} P[ X = d_{i}],
                \text{ } D \subset \mathcal{D}
            \end{equation*}

            Note that the notation $ P[ X = d_{i}] $ is an abbreviation, since
            the outcome $ c $ is not actually important here.

        \subsection{Cumulative Distribution Function}

            The \textit{cumulative distribution function}, $ F_{X}(x) $, of $ X
            $ is defined by

            \begin{equation*}
                F_{X}(x) = P_{X}((-\infty, x]) = P[\{ c: X(c) \leq x \}] = P (
                { X \leq x } ).
            \end{equation*}

            Cdf is also simply called the \textit{distribution function}.

        \subsection{Probability Density Function}

            A random variable $ X $ is said to be \textit{continuous} if
            its cdf $ F_{X}(x) $ is continuous for all $ x \in \mathbb{R} $.

            Let $ X $ be a continuous random variable with interval $
            \mathcal{D} \subset \mathbb{R} $ as space. The \textit{probability
            density function} of $ X $, $ f_{X}(x) $, is a function that
            satisfies

            \begin{equation*}
                F_{X}(x) = P(X \leq x) = \int_{-\infty}^{x} f_{X}(t) dt.
            \end{equation*}

            When there exits such a function $ f_{X}(x) $, $ X $ is also called an
            \textit{absolutely continuous} random variable.

            If $ f_{X}(x) $ is also continuous, we have

            \begin{equation*}
                 \frac{d}{dx} F_{X}(x) = f_{X}(x)
            \end{equation*}

            by the Fundamental Theorem of Calculus. Note that for any continuous
            random variable $ X $, there are no points of discrete mass, hence

            \begin{equation*}
                 P(X = x) = 0,
            \end{equation*}

            for all $ x \in \mathbb{R} $.

            From this, we can also infer that

            \begin{equation*}
                 P ( a < X \leq b ) = P ( a \leq X \leq b) = P ( a \leq X < b)
                     = P ( a < X < b)
            \end{equation*}

        \subsection{Different random variable can have the same cdf}

            Let $ X $ has be a random variable that stands for a real
            random number randomly choosed from the interval $ (0, 1) $,
            and we simply use the sample as the assigned number. In this case,
            the domain is $ \mathcal{D} = (0, 1) $. Assign a probability on $ X $,

            \begin{equation*}
                P_{X}[(a, b)] = b - a, \text{ for } 0 < a < b < 1
            \end{equation*}

            Then the pdf of $ X $ is

            \begin{equation*}
                 f_{X}(x) =  \left\{
                     \begin{array}{ll}
                         1 & 0 < x < 1 \\
                         0 & \text{elsewhere}
                     \end{array}
                 \right.
            \end{equation*}

            It's easy to show that the cdf is

            \begin{equation*}
                F_{X}(x) = P ( X \leq x ) = \left\{
                     \begin{array}{ll}
                         0 & \text{if } x < 0 \\
                         x & \text{if } 0 \leq x < 1 \\
                         1 & \text{if } x \geq 1
                     \end{array}
                 \right.
            \end{equation*}

            Now consider $ Y = 1 - X $,

            \begin{align*}
                F_{Y}(y) &= P(Y \leq y) = P(1 -X \leq y) = P(X \geq 1 - y) = 1 - P(X < 1 - y) \\
                         &=  \left\{
                             \begin{array}{ll}
                                 0 & \text{if } y < 0        \\
                                 y & \text{if } 0 \leq y < 1 \\
                                 1 & \text{if } 1 \leq y
                             \end{array}
                         \right.
            \end{align*}

            In this case, we said $ X $ and $ Y $ are equal in distribution and
            denote by $ X \stackrel{D}{=} Y $.


        \subsection{Expectation}

            The \textit{expectation} of $ X $ is defined by

            \begin{equation}
                E[X]=\left\{
                    \begin{array}{ll}
                        \sum x_i p(x_{i}) & \text{if } X \text{ is discrete with pmf } p(x) \text{, and }
                            \sum |x| p(x) < \infty \\
                        \int x f(x) dx & \text{if } X \text{ is continuous with pdf } f(x) \text{, and }
                            \int |x| f(x) dx < \infty
                    \end{array}
                \right.
            \end{equation}

            Expectation is also called \textit{mean}, or \textit{expected value},
            and mostly denoted by $ \mu $.

            The expection can reflect the transformation of random variable. Let $ Y = g(X) $, then

            \begin{align*}
                E(Y) = E ( g(X) ) &= \sum g(x) p(x) \\
                E(Y) = E ( g(X) ) &= \int g(x) f(x) dx
            \end{align*}

            The expection is linear with respect to random variable,

            \begin{equation*}
                E [ k_{1} g_{1}(X) + k_{2} g_{2}(X) ] = k_{1} E [ g_{1}(X) ] + k_{2} E [ g_{2}(X) ]
            \end{equation*}

        \subsection{Variance and Standard Deviation}

            Let $ X $ be a random variable with finite mean $ \mu $ and $ E [ (X -
            \mu)^{2} ] $ is also finite. The variance of $ X $ is defined by

            \begin{equation}
                var[ X ] = E [ (X - \mu)^{2} ]
            \end{equation}

            Variance is mostly denoted by $ \sigma^{2} $. The single $ \sigma $ is
            called the \textit{standard deviation}. The number $ \sigma $ is sometimes
            interpreted as a measure of the dispersion of the points of the space
            relative to the mean value $ \mu $.

            Note that

            \begin{align*}
                \sigma^2 &= E [ ( X - \mu )^{2} ] = E ( X^2 - 2 X \mu + \mu^{2} ) \\
                         &= E [ X^{2} ] - 2 \mu^{2} + \mu^{2} \\
                         &= E [ X^{2} ] - \mu^{2}
            \end{align*}

    \section{Random Vector}

        Consider two random variables $ X_{1} $ and $ X_{2} $ on the same
        sample space $ \mathcal{C} $, that they assign each element $ c $ of $
        \mathcal{C} $ one and only one ordered pair of numbers $ X_{1}(c) =
        x_{1} $ , $ X_{2}(c) = x_{2} $. Then we say that $ (X_{1}, X_{2}) $ is
        a random vector. The \textit{space} of $ (X_{1}, X_{2}) $ is the set
        of ordered pairs $ \mathcal{D} =  \{ (x_{1}, x_{2}) : x_{1} = X_{1}(c),
        x_{2} = X_{2}(c), c \in \mathcal{C} \} $.

        \subsection{Probability Mass Function}

            A discrete random vector $ (X_{1}, X_{2}) $ with finite or
            countable space $ \mathcal{D} $. The \textit{joint probability mass
            function} of $ (X_{1}, X_{2})$, $ p_{X_{1}, X_{2}}(x_{1}, x_{2}) $,
            is defined by

            \begin{equation*}
                p_{ X_{1}, X_{2}} (x_{1}, x_{2}) = P [ X_{1} = x_{1}, X_{2} = x_{2} ]
            \end{equation*}

            for all $ (x_{1}, x_{2}) \in \mathcal{D} $.

        \subsection{Cumulative Distribution Function}

            The cumulative distribution function of $ (X_{1}, X_{2}) $, $
            F_{X_{1}, X_{2}} (x_{1}, x_{2}) $, is defined by

            \begin{equation*}
                  F_{X_{1}, X_{2}} (x_{1}, x_{2}) = P [  \{ X_{1} \leq x_{1} \}
                  \cap  \{ X_{2} \leq x_{2} \}],
            \end{equation*}

            for all $ (x_{1}, x_{2}) \in \mathbb{R} $. This is also called
            \textit{joint cumulative distribution function}.

            We'll also abbreviate $
            P [  \{ X_{1} \leq x_{1} \} \cap  \{ X_{2} \leq x_{2} \}] $ to
            $ P [ X_{1} \leq x_{1}, X_{2} \leq x_{2} ] $.

        \subsection{Probability Density Function}

            A random vector $ (X_{1}, X_{2}) $ with space $ \mathcal{D} $ is
            said to be continuous if

            \begin{equation*}
                F_{X_{1}, X_{2}}(x_{1}, x_{2}) = P [  \{ X_{1} \leq x_{1} \}
                  \cap  \{ X_{2} \leq x_{2} \} ]
            \end{equation*}

            is continuous.

            The \text{joint probability density function} of $ (X_{1}, X_{2})
            $, $ f_{X_{1}, X_{2}}(x_{1}, x_{2}) $, is defined to satisfy

            \begin{equation*}
                 F_{X_{1}, X_{2}} (x_{1}, x_{2}) = \int_{-\infty}^{x_{1}}
                     \int_{-\infty}^{x_{2}} f_{X_{1}, X_{2}} (w_{1}, w_{2}) d w_{1} d w_{2}
            \end{equation*}

            for all $ (x_{1}, x_{2}) \in \mathbb{R} $. Then

            \begin{equation*}
                 \frac{\partial^{2} F_{X_{1}, X_{2}} (x_{1}, x_{2})}{\partial
                     x_{1} \partial x_{2}} = f_{X_{1}, X_{2}} (x_{1}, x_{2})
            \end{equation*}

            For an event $ A \subset \mathcal{D} $, we have

            \begin{equation*}
                 P [ (X_{1}, X_{2}) \in A ] = \int \int_{A} f_{X_{1}, X_{2}}
                     (x_{1}, x_{2}) d x_{1} d x_{2}
            \end{equation*}

        \subsection{Marginals}

            Let $ (X_{1}, X_{2}) $ be a random vector. Recall that

            \begin{align*}
                \{ X_{1} \leq x_{1} \} &= \{ c : X_{1}(c) \leq x_{1} \}
                    = \{ c: X_{1}(c) \leq x_{1} \} \cap \{ c: -\infty < X_{2} < \infty \} \\
                                       &= \{ X_{1} \leq x_{1}, -\infty < X_{2} < \infty \},
            \end{align*}

            hence,

            \begin{equation*}
                F_{X_{1}}(x_{1}) = P[ X_{1} \leq x_{1}, -\infty < X_{2} < \infty ],
            \end{equation*}

            for all $ x_{1} \in \mathbb{R} $. By the property of cdf, we can get

            \begin{equation*}
                 F_{X_{1}}(x_{1}) = \lim_{x_{2} \rightarrow \infty} F_{X_{1}, X_{2}} (x_{1}, x_{2}).
            \end{equation*}

            This is exactly where we connect the cdf, pdf, pmf between random
            variable and random vector.

            \subsubsection{Discrete}

                For discrete $ (X_{1}, X_{2}) $. Let $ \mathcal{D}_{X_{1}} $ be the support of $ X_{1} $, i.e.,
                $ \mathcal{D}_{X_{1}} = \{ x \in \mathcal{D} : p_{X_{1}}(x) \neq 0 \} $ where $ \mathcal{D} $ is
                the space of $ X_{1} $. For $ x_{1} \in \mathcal{D}_{X_{1}} $

                \begin{align*}
                    F_{X_{1}}(x_{1}) &= P [ X_{1} \leq x_{1}, -\infty < X_{2} < \infty ] \\
                                     &= \mathop{\sum \sum}_{\substack{w_{1} \leq x_{1}, -\infty < x_{2} < \infty }}
                                        p_{X_{1}, X_{2}}(w_{1}, x_{2}) \\
                                     &= \sum_{ w_{1} \leq x_{1}} \left\{ \sum_{ x_{2} < \infty}
                                            p_{X_{1}, X_{2}}(w_{1}, x_{2})
                                        \right\}
                \end{align*}

                By uniqueness of cdfs, we know the pmf of $ X_{1} $ must be

                \begin{equation*}
                     p_{X_{1}}(x_{1}) = \sum_{x_{2} < \infty} p_{X_{1}, X_{2}} (x_{1}, x_{2}),
                \end{equation*}

                for all $ x_{1} \in \mathcal{D}_{X_{1}} $. This is called the \textit{marginal pmf} of $ X_{1} $.
                We can get similar result for $ X_{2} $.

            \subsubsection{Continuous}

                For continuous $ (X_{1}, X_{2}) $. We use the same notation as the discrete one. Then

                \begin{equation*}
                    F_{X_{1}} (x_{1}) = \int_{ -\infty }^{ x_{1} } \int_{
                        -\infty }^{ \infty } f_{X_{1}, X_{2}} (w_{1}, x_{2}) d x_{2} d w_{1}
                        = \int_{ -\infty }^{ x_{1} } \left\{
                            \int_{ -\infty }^{ \infty } f_{X_{1}, X_{2}} (w_{1}, x_{2}) d x_{2}
                            \right\} d w_{1},
                \end{equation*}

                for all $ x_{1} \in \mathcal{D}_{X_{1}} $. The pdf of $ X_{1} $ must be

                \begin{equation*}
                     f_{X_{1}}(x_{1}) = \int_{ -\infty }^{ \infty } f_{X_{1}, X_{2}} (x_{1}, x_{2}) d x_{2}
                \end{equation*}

    \section{Random Sample}

\end{document}
