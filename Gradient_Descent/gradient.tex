\documentclass{article}
\usepackage[a4paper]{geometry}
\usepackage[utf8]{inputenc} 
\usepackage{indentfirst}
%list code
\usepackage{listings}
%content include references
\usepackage[nottoc,numbib]{tocbibind}
% table of contents clickabel
\usepackage{hyperref}
% make file system neat and easier to manage
\usepackage{subfiles}
% packages
\usepackage{csquotes}
\usepackage{arydshln}
\usepackage{dsfont}
\usepackage{polynom}
\usepackage{empheq}
\usepackage{calc}  
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{systeme}
\usepackage{mathtools}
\usepackage{tikz}
\usepackage{amsfonts, mathrsfs, bm, amsmath, amssymb, bbm, amsthm}
\usepackage{ifthen}
\usepackage{enumerate}
% Make the preview part more eye friendly
\usepackage{xcolor}
\pagecolor[rgb]{1,1,1} % background
\color[rgb]{0,0,0} % foreground
% new command
\DeclareMathOperator*{\argmax}{arg~max}
\DeclareMathOperator*{\argmin}{arg~min}
% theorem block
\newtheorem{theorem}{Theorem}[section]
\newtheorem{note}{Note}
% cross page equation
\allowdisplaybreaks

\begin{document}
    \section{Gradient Descent}
        \subsection{Vanilla Gradient Descent}
            Compute gradient for the entire training set and then update parameters
            \[
                \theta^t = \theta^{t-1} - \alpha \nabla L(\theta^{t-1})
            \]
        \subsection{Stochastic Gradient Descent}
            Compute gradient for one training data and then update parameters subsequently.
            \[
                \theta^t = \theta^{t-1} - \alpha\nabla L_i(\theta^{t-1})
            \]
            where $L_i$ means the loss function applies on only one instance.
            \subsubsection{Randomly Choose and Return}
                $\{x_1,\dots,x_N\}$, $L(\theta)=\frac{1}{N}\sum_{i=1}^N L_i(\theta)$. Choose $x_i$ uniformly randomly from $\{x_1,\dots,x_N\}$.
                \[
                    \theta^1 = \theta^{0}-\alpha\nabla L_i(\theta^0)
                \]
                $x_i$ will not be pulled out from instances. Choose new $x_j$ uniformly randomly again.
                \[
                    \theta^2 = \theta^1-\alpha\nabla L_j(\theta^1)
                \]
            \subsubsection{Shuffle and Not Return}
                Randomly shuffle $\{1,\dots,N\}$ into $\{k_1,\dots,k_N\}$. For $i$ for $1$ to $N$,
                \[
                    \theta^t = \theta^{t-1}-\alpha\nabla L_{k_i}(\theta^{i-1})
                \]
                One process go trough all instances is an "epoch".
        \subsection{Mini-Batch Gradient Descent}
            Split instances into $m$ subsets, each subset (or mini-batch) has $n$ instances. Update $\theta$ per mini-batch.
            \[
                \theta^t = \theta^{t-1}-\alpha \frac{1}{m}\sum_{i=1}^m\nabla L_{i}(\theta)
            \]
            After going through all batches, we complete one "epoch".
\end{document}
