\documentclass{article}
\usepackage[a4paper]{geometry}
\usepackage[utf8]{inputenc} 
\usepackage{indentfirst}
%list code
\usepackage{listings}
%content include references
\usepackage[nottoc,numbib]{tocbibind}
% table of contents clickabel
\usepackage{hyperref}
% make file system neat and easier to manage
\usepackage{subfiles}
% packages
\usepackage{csquotes}
\usepackage{arydshln}
\usepackage{dsfont}
\usepackage{polynom}
\usepackage{empheq}
\usepackage{calc}  
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{systeme}
\usepackage{mathtools}
\usepackage{tikz}
\usepackage{amsfonts, mathrsfs, bm, amsmath, amssymb, bbm, amsthm}
\usepackage{ifthen}
\usepackage{enumerate}
% Make the preview part more eye friendly
\usepackage{xcolor}
\pagecolor[rgb]{1,1,1} % background
\color[rgb]{0,0,0} % foreground
% new command
\DeclareMathOperator*{\argmax}{arg~max}
\DeclareMathOperator*{\argmin}{arg~min}
% theorem block
\newtheorem{theorem}{Theorem}[section]
\newtheorem{note}{Note}
% cross page equation
\allowdisplaybreaks

\begin{document}
    \section*{Calssification Loss Functions}

        \subsection*{Hinge Loss}

            Let $ g(x) $ be a classifier that defined by a score function $ f(x) $

            \begin{equation*}
                g(x) = \left\{ 
                         \begin{array}{rl}
                            1 & \mathrm{if~} f(x) > 0\\
                            -1 & \mathrm{if~} f(x) \leq 0
                         \end{array}
                       \right.
            \end{equation*}

            Suppose there are $ N $ data points $ x_1, \dots, x_N $ with labels
            $ \hat{y}_1,\dots,\hat{y}_N \in \{ -1, 1 \} $. The hinge loss of $ g $
            is defined to be

            \begin{equation*}
                l\big( g(x_n),\hat{y}_n \big) = \max\{ 0, 1 - \hat{y}_n f(x_n) \}
            \end{equation*}

            When $ f(x_n) \geq 1 $ or $ f(x_n) \leq -1 $, both implies $ \hat{y}_n
            f(x_n) \geq 1 $. This means $ l\big( g(x_n), \hat{y}_n \big) = 0 $.

            When $ f(x_n) \in (-1, 1) $, we have $ \hat{y}_n f(x_n)\in [0, 1) $.
            Hence $ l\big( g(x_n), \hat{y}_n \big) = 1 - \hat{y}_n f(x_n) $.

        \subsection*{Cross Entropy}

            Let $ p(x) $ be an unknown distribution and we use $ q(x) $ to esitmate
            it. The cross entropy of $ q(x) $ relative to $ p(x) $ is

            \begin{equation*}
                H(p,q) = -\mathrm{E}_p[\ln q] = - \sum_x p(x) \ln q(x) 
            \end{equation*}

    \section*{Regression Loss Functions}

        \subsection*{\texorpdfstring{$L^p$}{Lp} Norm}

            Let $\boldsymbol{y}(\boldsymbol{w},\boldsymbol{b}): \mathbb{R}^n
            \rightarrow \mathbb{R}^m$ be a model defined by $\boldsymbol{w}$
            and $\boldsymbol{b}$ that map $\boldsymbol{x} \in \mathbb{R}^n$
            into $\boldsymbol{y} \in \mathbb{R}^m$. Mathematically the $L^1$
            norm is

            \begin{equation*}
                \| \boldsymbol{y}(\boldsymbol{w},\boldsymbol{b}) -
                \hat{\boldsymbol{y}} \|_1 = \sum_{k=1}^m \big|
                y_k(\boldsymbol{w},\boldsymbol{b}) - \hat{y}_k \big| 
            \end{equation*}

            The $L^p$ norm is

            \begin{equation*}
                \| \boldsymbol{y}(\boldsymbol{w},\boldsymbol{d}) -
                \hat{\boldsymbol{y}}\|_p = \left( \sum_{k=1}^m
                \big(y_k(\boldsymbol{w},\boldsymbol{b})-\hat{y}_k\big)^p \right)^{1/p}
            \end{equation*}

            The $L^\infty$ norm is

            \begin{equation*}
                \| \boldsymbol{y}(\boldsymbol{w}, \boldsymbol{b}) -
                \hat{\boldsymbol{y}}\|_{\infty} = \max_{k}
                |y_k(\boldsymbol{w},\boldsymbol{b}) - \hat{y}_k| 
            \end{equation*}

        \subsection*{Mean Absolute Error(\texorpdfstring{$L^1$}{L1} Loss)}

            Suppose there are $N$ instances $\{ \boldsymbol{x}_i \}_{i=1}^N$. The MAE is defined by

            \begin{equation*}
                L(\boldsymbol{w},\boldsymbol{b}) = \frac{\sum_{n=1}^N \|
                \boldsymbol{y}^n(\boldsymbol{w},\boldsymbol{b}) -
                \hat{\boldsymbol{y}^n} \|_1}{N}
            \end{equation*}

        \subsection*{Mean Square Error(\texorpdfstring{$L^2$}{L2} Loss)}

            Suppose there are $N$ instances $\{ \boldsymbol{x}_i \}_{i=1}^N$. The MSE is defined by

            \begin{equation*}
                L(\boldsymbol{w},\boldsymbol{b}) = \frac{\sum_{n=1}^N \|
                \boldsymbol{y}^n(\boldsymbol{w},\boldsymbol{b}) -
                \hat{\boldsymbol{y}^n}\|_2^2}{N}
            \end{equation*}

\end{document}
