\documentclass[../main.tex]{subfiles}

\begin{document}
    \section{Simple Regression Model}
        \subsection{Linear}
            \subsubsection{One Dimension}
                Each instance has one attribute $x$ corresponds to one label $y$. 
                \[
                    y=wx+b,\quad y,~x,~w,~b\in\mathbb{R}
                \]
            \subsubsection{Multiple Dimensions}
                Each instance has multiple attributes $x_1,\dots,x_n$ and multiple labels $y_1,\dots,y_m$.
                \begin{align*}
                    &\qquad\boldsymbol{y}=\boldsymbol{wx}+\boldsymbol{b},\\
                    &\qquad\boldsymbol{x}\in\mathbb{R}^n, \\
                    &\qquad\boldsymbol{w}\in\mathbb{R}^{m\times n},\\
                    &\qquad\boldsymbol{y},~\boldsymbol{b}\in\mathbb{R}^m,\\
                    \begin{pmatrix}
                        y_1\\
                        \vdots\\
                        y_m
                    \end{pmatrix}=&\begin{pmatrix}
                        w_{11} & \dots & w_{1_n}\\
                        \vdots & \ddots & \vdots\\
                        w_{m1} & \dots & w_{mn}
                    \end{pmatrix}\begin{pmatrix}
                        x_1\\
                        \vdots\\
                        x_n
                    \end{pmatrix}+\begin{pmatrix}
                        b_1\\
                        \vdots\\
                        b_m
                    \end{pmatrix}
                \end{align*}

        \subsection{Nonlinear(Polynomial Model)}
            \subsubsection{One Dimension}
                Each instance has one attribute $x$ correponds to one label $y$.
                \begin{align*}
                    y&=w_2x^2+w_1x+b, \\
                    &\quad \vdots\\
                    y&=w_kx^k+w_{k-1}x^{k-1}+\cdots+w_1x+b
                \end{align*}
            \subsubsection{Multiple Dimensions}
                Each instance has multiple attributes $x_1,\dots,x_n$ and multiple labels $y_1,\dots,y_m$.
                \begin{align*}
                    \boldsymbol{y}&=\boldsymbol{w}_2\boldsymbol{x}^2+\boldsymbol{w}_1\boldsymbol{x}+\boldsymbol{b}, \\
                    &\quad \vdots\\
                    \boldsymbol{y}&=\boldsymbol{w}_k\boldsymbol{x}^k+\boldsymbol{w}_{k-1}\boldsymbol{x}^{k-1}+\dots+\boldsymbol{w}_1\boldsymbol{x}+\boldsymbol{b}
                \end{align*}
            where
                \[
                    \boldsymbol{x}^p=\left(\begin{array}{c}
                        x_1^p\\
                        \vdots\\
                        x_n^p
                    \end{array}\right),~
                    \boldsymbol{w}_p \in \mathbb{R}^{m \times n},~
                    \boldsymbol{y}, \boldsymbol{b} \in \mathbb{R}^m
                \]

    \section{Logistic Regression}
        Put the function of linear regression into sigmoid function, the output value will lie in $(0,1)$.
        \[
            f_{w,b}(x)=\sigma(w \cdot x + b)=\sigma\left(\sum_i w_ix_i+b\right)
        \]
        In the training set $\{(x_k,\hat{y}_k)\}_k$, $\hat{y}_k\in\{0,1\}$. $1$ for class $C_1$, $0$ for class $C_2$. If $(x_1,x_2,x_3,\dots)$ corresponds to $(1,1,0,\dots)$. The loss function
        \[
            L(w,b) = f_{w,b}(x_1)f_{w,b}(x_2)\big(1-f_{w,b}(x_3)\big)\dots,\quad w^*,b^*=\argmax_{w,b}L(w,b)
        \]
        Note that
        \[
            w^*,b^*=\argmin -\ln L(w,b)
        \]
        And
        \begin{align*}
            -\ln L(w,b) &= -\ln f_{w,b}(x_1)-\ln f_{w,b}(x_2)-\ln(1-f_{w,b}(x_3))\dots\\
                        &= \sum_{k}-\left[\hat{y}_k \ln f_{w,b}(x_k)+(1-\hat{y}_k)\ln \big(1-f_{w,b}(x_k)\big)\right]
        \end{align*}
        The relation in the brackets $[~]$ is called the cross entropy between two Bernoulli distribution.
        
        \subsection{Comparison with Linear Regression}
            Simple computation shows that
            \[
                \frac{\partial -\ln L(w,b)}{\partial w_i}=\sum_k -\big(\hat{y}_k-f_{w,b}(x_k)\big)x_{k,i}
            \]
            So $w_i$ will update in the same way with linear regression. The only difference between them is the range of output. Logistic regression lies in $(0,1)$ while linear regression can be any real number.
\end{document}
    
