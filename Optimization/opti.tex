\documentclass{article}
\usepackage[a4paper]{geometry}
\usepackage[utf8]{inputenc} 
\usepackage{indentfirst}
%list code
\usepackage{listings}
%content include references
\usepackage[nottoc,numbib]{tocbibind}
% table of contents clickabel
\usepackage{hyperref}
% make file system neat and easier to manage
\usepackage{subfiles}
% packages
\usepackage{csquotes}
\usepackage{arydshln}
\usepackage{dsfont}
\usepackage{polynom}
\usepackage{empheq}
\usepackage{calc}  
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{systeme}
\usepackage{mathtools}
\usepackage{tikz}
\usepackage{amsfonts, mathrsfs, bm, amsmath, amssymb, bbm, amsthm}
\usepackage{ifthen}
\usepackage{enumerate}
% Make the preview part more eye friendly
\usepackage{xcolor}
\pagecolor[rgb]{1,1,1} % background
\color[rgb]{0,0,0} % foreground
% new command
\DeclareMathOperator*{\argmax}{arg~max}
\DeclareMathOperator*{\argmin}{arg~min}
% theorem block
\newtheorem{theorem}{Theorem}[section]
\newtheorem{note}{Note}
% cross page equation
\allowdisplaybreaks

\begin{document}
    \section{Optimization}
        \subsection{Hessian Matrix}
            Let $f:\mathbb{R}^n\rightarrow\mathbb{R}$ be a twice differentiable function, its Hessian matrix at $x\in\mathbb{R}^n$ is defined as
            \[
                \nabla^2f(x)=
                \begin{pmatrix}
                    \frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1x_2} & \frac{\partial^2ff}{\partial x_1x_3}&\cdots&\frac{\partial^2 f}{\partial x_1x_n}\\[0.2cm]
                    \frac{\partial^2 f}{\partial x_2x_1} & \frac{\partial^2 f}{\partial x_2^2} & \frac{\partial^2 f}{\partial x_2x_3} & \cdots & \frac{\partial^2 f}{\partial x_2x_n}\\[0.2cm]
                    \vdots & \vdots &\vdots & \ddots &\vdots\\[0.2cm]
                    \frac{\partial^2 f}{\partial x_nx_1} & \frac{\partial^2 f}{\partial x_nx_2} & \frac{\partial^2 f}{\partial x_nx_3} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
                \end{pmatrix}
                \in\mathbb{R}^{n \times n}
            \]
            \subsubsection{Quadratic Function}
                Let $f:\mathbb{R}^n\rightarrow\mathbb{R}$ and $f(x)=\frac{1}{2}x^THx+p^Tx$ where $H \in \mathbb{R}^{n \times n}$ is a symmetric matrix and $p \in \mathbb{R}^n$. Then
                \begin{align*}
                    \nabla f(x) &= Hx+p \\
                    \nabla^2 f(x) &= H
                \end{align*}
                If $H$ is positive definite, then $x^*=-H^{-1}p$ is a unique solution.
            
            \subsubsection{Least-Square Problem}
                \[
                    \min_{x \in \mathbb{R}^n}\|Ax - b\|_2^2,~A \in \mathbb{R}^{m \times n},~b \in \mathbb{R}^m
                \]
                \begin{align*}
                    f(x) &= (Ax - b)(Ax - b)^T \\
                         &= x^TA^TAx - 2b^TAx + b^Tb \\
                    \nabla f(x) &= 2A^TAx - 2A^Tb \\
                    \nabla^2 f(x) &= 2A^TA \\
                    x^* &= (A^TA)^{-1}A^Tb
                \end{align*}
            \subsubsection{Newton's Method}
                Compute $d^i$ satisfies
                \[
                    \nabla^2 f(x^i)d^i = -\nabla f(x^i)
                \]
                Update
                \[
                    x^{i+1} = x^i + d^i
                \]
                Until $\nabla f(x^i)=0$.
        \subsection{Lagrangian Function}
            For the problem
            \begin{align*}
                &\min_{x\in\Omega} f(x)\\
                &s.t.\quad g_i(x)\leq 0,~h_i(x)=0,~\forall~i=1,\dots,m
            \end{align*}
            Write $g(x)=(g_1(x),\dots,g_m(x))^T$ and $h(x)=(h_1(x),\dots,h_m(x))^T$.
            Let 
            \[
                \mathcal{L}(x,\alpha,\beta)=f(x)+\alpha^Tg(x)+\beta^Th(x) \mathrm{~and~} \alpha\in\mathbb{R}^m\geq 0
            \]
            For a fixed $\alpha\geq 0$ and a fixed $\beta$, if $\Bar{x}\in\argmin\{\mathcal{L}(x,\alpha,\beta)|x\in\mathbb{R}^n\}$ then
            \[
                \left.\frac{\partial\mathcal{L}(x,\alpha,\beta)}{\partial x}\right|_{x=\Bar{x}}=\nabla f(\Bar{x}) + \alpha^T\nabla g(\Bar{x})+\beta^T\nabla h(\Bar{x})=0
            \]
            
            \subsubsection{Duality}
                Consider
                \[
                    \max_{\alpha,\beta}\min_{x\in\Omega}\mathcal{L}(x,\alpha,\beta) \mathrm{~s.t.~}\alpha\geq0
                \]
                Let $\theta(\alpha,\beta)=\inf_{x\in\Omega}\mathcal{L}(x,\alpha,\beta)$. Then
                \[
                    \max_{\alpha,\beta}\theta(\alpha,\beta)\mathrm{~s.t.~}\alpha\geq 0
                \]
                For any $\Bar{x}\in\Omega$ and $(\Bar{\alpha}\geq0,\Bar{\beta})$ be solutions of the above problems, respectively, since $\Bar{\alpha}^T g(\Bar{x})\leq 0$ and $h(\Bar{x})=0$, we have $f(\Bar{x})\geq \theta(\Bar{\alpha},\Bar{\beta})$. 
                \begin{theorem}
                    If the equality happens, the $\Bar{x}$ and $(\Bar{\alpha}\geq 0, \Bar{\beta})$ solve the primal and dual problem, respectively. In this case,
                    \[
                        \mathbf{0}\leq \Bar{\alpha} \perp g(x)\leq\mathbf{0}
                    \]
                    Furthermore, for these $\Bar{x}$ and $(\Bar{\alpha}, \Bar{\beta})$,
                    \begin{align*}
                        f(\Bar{x}) &= \theta(\Bar{\alpha},\Bar{\beta})\\
                                   &= \inf_{x \in \Omega}\{f(x) + \Bar{\alpha}^T g(x) + \Bar{\beta}^T h(x)\}\\
                                   &\leq f(\Bar{x}) + \Bar{\alpha}^T g(\Bar{x}) + \Bar{\beta}^T h(\Bar{x})\\
                                   &= f(\Bar{x}) + \Bar{\alpha}^T g(\Bar{x})\\
                                   &\leq f(\Bar{x})
                    \end{align*}
                    This implies that 
                    \[
                        \Bar{\alpha}^T g(\Bar{x}) = 0
                    \]
                \end{theorem}
            
            \subsubsection{Karush-Kuhn-Tucker Condition(KKT Condition)}
                This is a summary of solve both primal form and dual form. Find $\Bar{x}\in\Omega$, $\Bar{\alpha},\Bar{\beta}\in\mathbb{R}^m$ such that
                \begin{align*}
                    \mathbf{Stationarity \quad} & \nabla f(\Bar{x}) + \Bar{\alpha}^T\nabla g(\Bar{x}) + \Bar{\beta}^T\nabla h(\Bar{x})=0\\
                    \mathbf{Complmentary~Slackness \quad} & \Bar{\alpha}^T g(\Bar{x})=0\\
                    \mathbf{Primal~Feasibility \quad} & h(\Bar{x})=0,~g(\Bar{x})\leq 0\\
                    \mathbf{Dual~Feasibility \quad} & \Bar{\alpha}\geq 0
                \end{align*}
                
            \subsubsection{Dual Linear Problem}
                For the primal linear problem
                \[
                    \min_{x\in\mathbb{R}^n}p^Tx\mathrm{\quad s.t.\quad}Ax\geq b,~x\geq 0
                \]
                Consider
                \[
                    \max_{\alpha_1,\alpha_2\geq \mathbf{0}}\min_{x\in\mathbb{R}^n}\mathcal{L}(x,\alpha,\beta)=p^Tx+\alpha_1^T(b-Ax)+\alpha_2^T(-x)
                \]
                When $\Bar{x}\in\mathrm{arg~min}\{\mathcal{L}(x,\alpha,\beta)|x\in\Omega\}$, the gradient $\nabla\mathcal{L}(\Bar{x},\alpha,\beta)$ vanish
                \[
                    p-A^T\alpha_1-\alpha_2=0
                \]
                Then we have the dual problem
                \[
                    \max_{\alpha_1,\alpha_2\geq \mathbf{0}}b^T\alpha_1\mathrm{\quad s.t. \quad}p-A^T\alpha_1-\alpha_2=0
                \]
                Since $\alpha_2$ is a slack variable, it's equivalent to
                \[
                    \max b^T\alpha\mathrm{\quad s.t. \quad}A^T\alpha\leq p,~\alpha\geq 0
                \]
            \subsubsection{Least Square Problem}
                For $\min_{x\in\mathbb{R}^n}\|Ax-b\|_2^2$ where $A\in\mathbb{R}^{m\times n}$ and $b\in\mathbb{R}^m$. It's obvious that
                \[
                    x^*\in\big\{ \argmin\{\|Ax-b\|_2^2 \big\}\Rightarrow A^TAx = A^Tb
                \]
                Consider the problem
                \[
                    \min \mathbf{0}^Tx\mathrm{\quad s.t. \quad}A^TAx=A^Tb
                \]
                Then for
                \[
                    \max_{\alpha}\min \mathbf{0}^Tx+\alpha^T(A^Tb-A^TAx)\mathrm{\quad s.t. \quad}\alpha\in\mathbb{R}^m
                \]
                and the dual problem
                \[
                    \max_{\alpha} b^T A \alpha \mathrm{\quad s.t. \quad} (A^TA)^T\alpha=\mathbf{0}
                \]
                The constraint has a trivial solution $\alpha=\mathbf{0}$ and the objective function has value $0$. The objective function of the primal problem and the dual problem have the same value. This implies that $A^TAx=A^Tb$ must have a solution. Otherwise the dual problem won't have optimal solution.
            \subsubsection{Quadratic Problem}
                The primal problem
                \[
                    \min_{x\in\mathbb{R}^n} \frac{1}{2} x^T Q x + p^T x \mathrm{\quad s.t. \quad} Ax\leq b
                \]
                For
                \[
                    \max_{\alpha\geq 0}\min_{x\in\mathbb{R}^n} \frac{1}{2} x^T Q x + p^T x + \alpha^T (Ax - b) \mathrm{\quad s.t. \quad} \alpha \geq 0
                \]
                the gradient needs vanishing
                \[
                    Qx + p + A^T \alpha = 0 \Rightarrow x = -Q^{-1}(A^T \alpha + p)
                \]
                Substitute back and we have the dual form
                \[
                    \max -\frac{1}{2}(p^T + \alpha^T A) Q^{-1} (A^T\alpha + p) - \alpha^T b \mathrm{\quad s.t. \quad} \alpha \geq \mathbf{0} 
                \]
\end{document}
