\documentclass{article}
\usepackage[a4paper]{geometry}
\usepackage[utf8]{inputenc} 
\usepackage{indentfirst}
%list code
\usepackage{listings}
%content include references
\usepackage[nottoc,numbib]{tocbibind}
% table of contents clickabel
\usepackage{hyperref}
% make file system neat and easier to manage
\usepackage{subfiles}
% packages
\usepackage{csquotes}
\usepackage{arydshln}
\usepackage{dsfont}
\usepackage{polynom}
\usepackage{empheq}
\usepackage{calc}  
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{systeme}
\usepackage{mathtools}
\usepackage{tikz}
\usepackage{amsfonts, mathrsfs, bm, amsmath, amssymb, bbm, amsthm}
\usepackage{ifthen}
\usepackage{enumerate}
% Make the preview part more eye friendly
\usepackage{xcolor}
\pagecolor[rgb]{1,1,1} % background
\color[rgb]{0,0,0} % foreground
% new command
\DeclareMathOperator*{\argmax}{arg~max}
\DeclareMathOperator*{\argmin}{arg~min}
% theorem block
\newtheorem{theorem}{Theorem}[section]
\newtheorem{note}{Note}
% cross page equation
\allowdisplaybreaks

% https://arxiv.org/abs/1910.03558
% https://www.amazon.com/Optimization-Vector-Space-Methods-Luenberger/dp/047118117X

\begin{document}

    \section*{Problem Setup}

        We have an unknown target state $ x_{k} \in \mathbb{R}^{n} $, we want
        to estimate it during each timestamp $ k \in \mathbb{N} \cup \{0\} $.
        We create some measurement method to help us guess each $ x_{k} $. The
        Kalman Filter's objective is to guess a theoretically perfect $ x_{k} $
        from each measurement $ z_{k} $.

        TL;DR, the Kalman Filter is the process doing: get $ z_{1} $, guess $
        x_{1} $; get $ z_{2} $, guess $ x_{2} $; get $ z_{3} $, guess $ x_{3}
        $, and so on.

    \section{Assumptions}

        By means of \textit{theoretically}, we need some assumptions.

        \subsection{Pre-Defined Transition}

            Let $ F_{k} \in \mathbb{R}^{n \times n} $ be a matrix. We assume
            that the next state $ x_{k+1} $ can be obtained from the current
            state $ x_{k} $ by

            \begin{equation}
                 x_{k+1} = F_{k}x_{k} + u_{k}
            \end{equation}

            where $ u_{k} \sim \mathcal{N}(0, Q_{k}) $  is a white noise, $
            u_{k} \in \mathbb{R}^{n} $, and $ Q_{k} \in \mathbb{R}^{n \times n}
            $.

            This is also called \textit{the linear model of Kalman Filter}.

        \subsection{Measurement to Real State}

            Let $ H_{k} \in \mathbb{R}^{m \times n} $ be a matrix. We assume
            that the current measurement $ z_{k} \in \mathbb{R}^{m} $ is
            obtained from the current state $ x_{k} $ by

            \begin{equation}\label{eq:measurement}
                 z_{k} = H_{k} x_{k} + w_{k}
            \end{equation}

            where $ w_{k} \sim \mathcal{N}(0, R_{k}) $ is a white noise, $ w_{k} \in
            \mathbb{R}^{m} $, and $ R_{k} \in \mathbb{R}^{m \times m} $.

    \section{Initial Setup}

        We never know $ x_{k} $, we can only guess it, so denote the guessed value
        by $ \hat{x}_{k} $. Formally, we call $ \hat{x}_{k} $ the \textit{estimate}.

        The zero-th step of the Kalman Filter is randomly guess $ \hat{x}_{0} $
        since there's no $ z_{-1} $. We further concern the correctness of each
        $ \hat{x}_{k} $, we use the error covariance matrix $ P_{k} =
        \operatorname{E}[(x_{k} - \hat{x}_{k})(x_{k} - \hat{x}_{k})^{T}] $ to
        formally compute it. So when we start with $ x_{0} $, we'll also have
        another randomly guess matrix $ P_{0} $.

        TL;DR, the initial setup is one
        vector $ \hat{x}_{0} $ and one matrix $ P_{0} $.

    \section{Preliminary Theorems}

        \subsection{Minimum Variance Unbiased Estimate}

            Consider the measurement equation \eqref{eq:measurement}, we wanna seek
            a matrix $ K_{k} \in \mathbb{R}^{n \times m} $ that can do the guessing
            $ \hat{x}_{k} = K_{k}z_{k} $. Since $ w_{k} $ is a random vector, $
            z_{k} $ is also a random vector, then so does $ \hat{x}_{k} $. As a
            result, the error term $ \hat{x}_{k} - x_{k} $ is a random vector as
            well. In this subsection's discussion, the timestamp subscript $ k $
            won't effect any computation, so I ignore it to avoid the confusion.

            We can define an inner product of two random vectors be

            \begin{equation*}
                (x|y) = \operatorname{E}(x^{T} y) = \operatorname{E}\left(
                \sum_{i=1}^{n} x_{i} y_{i} \right),
            \end{equation*}

            and then induces a norm by

            \begin{equation*}
                 \| x \| = \sqrt{\operatorname{E}(x^{T}x)} =
                 \sqrt{\operatorname{E}(x_{1}^{2} + \cdots + x_{n}^{2})} = 
                     \left\{ \operatorname{Tr}\left( \operatorname{E}(x x^{T})
                     \right) \right\}^{1/2}.
            \end{equation*}

            Now, consider the l2-norm

            \begin{align*}
                 &\| \hat{x} - x \|^{2} = \operatorname{E}\left[ (\hat{x} - x)^{T} (\hat{x} - x) \right] \\
                =&\operatorname{E}\left[ \left( Kz - x \right)^{T} \left( Kz - x \right) \right] \\
                =&\operatorname{E}\left[ \left( K \left( Hx + w \right) - x \right)^{T} \left( K\left( Hx + w \right) - x \right) \right] \\
                =&\operatorname{E}\left[ \left( (KHx)^{T} + (Kw)^{T} - x^{T} \right) \left( K\left( Hx + w \right) - x \right) \right] \\
                =&\operatorname{E}\left[ \left( KHx - x \right)^{T} \left( KHx - x \right) \right] + \operatorname{E}\left[ \left( Kw \right)^{T} (Kw) \right] \\
                =&\|KHx - x\|^{2} + \operatorname{Tr}\left( \operatorname{E}\left[ Kw\left( Kw \right)^{T} \right] \right) \\
                =&\|KHx - x\|^{2} + \operatorname{Tr}\left( K \operatorname{E}[w w^{T}] K^{T} \right) \\
                =&\|KHx - x\|^{2} + \operatorname{Tr}\left( K R K^{T} \right).
            \end{align*}

            The final term still depends on the actual state $ x $, but we
            never know its value. To solve this, we add one more assumption $
            KH = I $. Observe that if $ KH = 1 $, we have

            \begin{equation*}
                \operatorname{E}[\hat{x}] = \operatorname{E}\left[
                    Kz \right] = \operatorname{E}\left[ KHx +
                Kw \right] = \operatorname{E}\left[ KHx \right]
                + \operatorname{E}\left[ Kw \right] = x,
            \end{equation*}

            this implies that $ \hat{x} $ is an unbiased estimate of $ x $.

            Then the error term becomes quite simple when we assume $ KH = 1 $

            \begin{equation*}
                 \| \hat{x} - x \|^{2} = \operatorname{E}\left[
                 (\hat{x} - x)^{T} (\hat{x} - x) \right] =
                 \operatorname{Tr}\left( K R K^{T} \right)
            \end{equation*}

            Our mission now becomes

            \begin{align*}
                & \argmin_{K} \operatorname{Tr}\left( K R K^{T} \right) \\
                & \operatorname{s.t.} KH = I_{n}
            \end{align*}

            This is an optimization problem with $ n \times n $ constraints.

            \# Proof Needed, ref: Ch4.4 in Optimization-Vector-Space-Methods-Luenberger

    \section{Main Theorem}

        \begin{theorem}

            The optimal estimate $ \hat{x}_{k+1} $ and $ P_{k+1} $ can be generated
            recursively as

            \begin{align*}
                \hat{x}_{k+1} &= F_{k} \hat{x}_{k} + F_{k} P_{k} H_{k}^{T}\left[
                 H_{k} P_{k} H_{k}^{T} + R_{k} \right]^{-1}\left( z_{k} - H_{k} \hat{x}_{k} \right) \\
                P_{k+1} &= F_{k}P_{k}\left\{ I - H_{k}^{T}\left[ H_{k} P_{k}
                    H_{k}^{T} + R_{k} \right]^{-1} H_{k} P_{k} \right\} F_{k}^{T} +
                    Q_{k}
            \end{align*}

        \end{theorem}

        \begin{proof}

            Suppose we have $ \hat{x}_{k-1} $ and $ P_{k-1} $. At $ k $, we obtain
            a new measurement

            \begin{equation*}
                 z_{k} = H_{k} x_{k} + w_{k}
            \end{equation*}

            which gives us additional information about $ x_{k} $.

        \end{proof}

\end{document}
