\documentclass{article}
\usepackage[a4paper]{geometry}
\usepackage[utf8]{inputenc} 
\usepackage{indentfirst}
%list code
\usepackage{listings}
%content include references
\usepackage[nottoc,numbib]{tocbibind}
% table of contents clickabel
\usepackage{hyperref}
% make file system neat and easier to manage
\usepackage{subfiles}
% packages
\usepackage{csquotes}
\usepackage{arydshln}
\usepackage{dsfont}
\usepackage{polynom}
\usepackage{empheq}
\usepackage{calc}  
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{systeme}
\usepackage{mathtools}
\usepackage{tikz}
\usepackage{amsfonts, mathrsfs, bm, amsmath, amssymb, bbm, amsthm}
\usepackage{ifthen}
\usepackage{enumerate}
% Make the preview part more eye friendly
\usepackage{xcolor}
\pagecolor[rgb]{1,1,1} % background
\color[rgb]{0,0,0} % foreground
% new command
\DeclareMathOperator*{\argmax}{arg~max}
\DeclareMathOperator*{\argmin}{arg~min}
% theorem block
\newtheorem{theorem}{Theorem}[section]
\newtheorem{note}{Note}
% cross page equation
\allowdisplaybreaks

\begin{document}

    \section*{Goal}

        Say $ X \sim P_{true} $ but $ P_{true} $ unknown. Our goal is to
        leverage $ X $ to generate more samples like they are also generated
        from $ P_{true} $.
        
        In reality, $ X $ can be tons of images, tons of text, tons of paragraph,
        or tons of videos, etc.

    \section*{Maximum Log Likelihood Approach}

        If $ P_{true} $ can be paremetrized by a parameter set $ \theta $.
        We can leverage the maximum likelihood to estimate the $ \theta $ that
        yields the "best fit" probability to the given samples.

        Suppose we have $ N $ real samples $ \{ x_{1}, \dots, x_{N} \} \sim P_{true} $.
        Suppose they are i.i.d from different Gaussians

        \begin{equation*}
            f( x_{i}; \mu, \sigma ) = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{(x_{i} - \mu)^2}{2 \sigma^2}}
        \end{equation*}

        The log likelihood function is

        \begin{align*}
             L( \{ x_{i} \}; \mu, \sigma ) &= \ln \prod_{i=1}^N f( x_{i}; \mu, \sigma ) \\
                             &= \sum_{i = 1}^N \ln f( x_{i}; \mu, \sigma )
        \end{align*}

        Solve

        \begin{align*}
              & \max_{\mu, \sigma} \sum_{i = 1}^N \ln f( x_{i}; \mu, \sigma ) \\
            = & \max_{\mu, \sigma} \sum_{i = 1}^N -\frac{( x_{i} - \mu ) ^ 2}{2
            \sigma^2} - \ln \sqrt{2 \pi \sigma^2 }
        \end{align*}

        is the classical approach. However, there's no efficient
        and simple probability model for larger dimensions.

    \section*{GAN}

        Min-Max Problem

        \begin{equation*}\label{ce-gan}
            \min_{G} \max_{D} E_{ x \sim P_{true} }[ \ln D(x) ] + E_{ z \sim P(z) }[ 1 - \ln
            D(G(z)) ]
        \end{equation*}

        $ x \sim P_{true} $ and $ z \sim P(z) $ implies Monte-Carlo

        \begin{align*}
            & E[ \ln D(X) ] = \frac{1}{N}\sum_{ i = 1 }^N \ln D(x_i) \\
            & E[ 1 - \ln D(G(Z)) ] = \frac{1}{N}\sum_{ i = 1 }^M \ln D(G(z_i))
        \end{align*}
            

    \section*{WGAN}

        For $ G $ fixed, the optimal $ D $ is

        \begin{equation*}
            D_G (x) = \frac{ P_{true}(x) }{ P_{true} (x) + P_{G} (x)}
        \end{equation*}

        KL divergence

        \begin{align*}
            KL(p \| q) = \sum_{i} p_{i} \frac{ \ln p_{i}}{ q_{i} } = E_{p} [ \ln \frac{p}{q} ]
        \end{align*}

        Jensenâ€“Shannon divergence

        \begin{align*}
            & JSD(p, q) =  \frac{1}{2} KL(p \| m) + \frac{1}{2} KL(q \| m), \\
            & m = \frac{1}{2} ( p + q)
        \end{align*}

        Let $ p, q $ be $ P_{true}, P_{G}$,

        \begin{equation*}
            \min_{G} JSD( P_{true}, P_{G})
        \end{equation*}

        Earth-Mover distance / Wasserstein-1 distance

        \begin{align*}
           & WS( P_{true}, P_{G} ) = \sup_{ \| D \|_{L} \leq 1 } \{ E_{x \sim
           P_{true} }[D(x)] - E_{ y \sim P_{G} }[D(y)] \}
        \end{align*}

        where $ D $ is 1-Lipschitz continuous, i.e.

        \begin{equation}
            \| D(x) - D(y) \| \leq \| x - y \|
        \end{equation}
        
    
\end{document}

