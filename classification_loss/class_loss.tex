\documentclass{article}
\usepackage[a4paper]{geometry}
\usepackage[utf8]{inputenc} 
\usepackage{indentfirst}
%list code
\usepackage{listings}
%content include references
\usepackage[nottoc,numbib]{tocbibind}
% table of contents clickabel
\usepackage{hyperref}
% make file system neat and easier to manage
\usepackage{subfiles}
% packages
\usepackage{csquotes}
\usepackage{arydshln}
\usepackage{dsfont}
\usepackage{polynom}
\usepackage{empheq}
\usepackage{calc}  
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{systeme}
\usepackage{mathtools}
\usepackage{tikz}
\usepackage{amsfonts, mathrsfs, bm, amsmath, amssymb, bbm, amsthm}
\usepackage{ifthen}
\usepackage{enumerate}
% Make the preview part more eye friendly
\usepackage{xcolor}
\pagecolor[rgb]{1,1,1} % background
\color[rgb]{0,0,0} % foreground
% new command
\DeclareMathOperator*{\argmax}{arg~max}
\DeclareMathOperator*{\argmin}{arg~min}
% theorem block
\newtheorem{theorem}{Theorem}[section]
\newtheorem{note}{Note}
% cross page equation
\allowdisplaybreaks

\begin{document}
    \section{Calssification Loss Functions}
        \subsection{Hinge Loss}
            Let $g(x)$ be a classifier that defined by a score function $f(x)$
            \[
                g(x) = \left\{ 
                         \begin{array}{rl}
                            1 & \mathrm{if~} f(x) > 0\\
                            -1 & \mathrm{if~} f(x) < 0
                         \end{array}
                       \right.
            \]
            Suppose there are $N$ data points $x_1,\dots,x_N$ with labels $\hat{y}_1,\dots,\hat{y}_N \in \{ -1, 1 \}$. The hinge loss of $g$ is defined to be
            \[
                l\big( g(x_n),\hat{y}_n \big) = \max\{ 0, 1 - \hat{y}_n f(x_n) \}
            \]
            When $f(x_n) \geq 1$ or $f(x_n) \leq -1$, both implies $\hat{y}_n f(x_n) \geq 1$. This means $l\big( g(x_n), \hat{y}_n \big) = 0$.
            When $f(x_n) \in (-1, 1)$, we have $\hat{y}_n f(x_n)\in [0, 1)$. Hence $l\big( g(x_n), \hat{y}_n \big) = 1 - \hat{y}_n f(x_n)$.
        \subsection{Cross Entropy}
            Let $p(x)$ be an unknown distribution and we use $q(x)$ to esitmate it. The cross entropy of $q(x)$ relative to $p(x)$ is
            \[
                H(p,q) = -\mathrm{E}_p[\ln q] = - \sum_x p(x) \ln q(x)
            \]
            Roughly speaking, this stands for the average amount of information to transimit when we use $q(x)$ as $p(x)$. Compare this with only use $p(x)$,
            the additional information will be transimitted is called the KL divergenc between $p(x)$ and $q(x)$
            \[
                \mathrm{KL}(p \| q) = H(p,q) - H(p) = \left( -\sum_x p(x) \ln q(x) \right) - \left( -\sum_x p(x) \ln p(x) \right)
            \]
            where $H(p)=-\sum_x p(x) \ln p(x)$ is the entropy of $p(x)$ alone.
\end{document}
