\documentclass{article}
\usepackage[a4paper]{geometry}
\usepackage[utf8]{inputenc} 
\usepackage{indentfirst}
%list code
\usepackage{listings}
%content include references
\usepackage[nottoc,numbib]{tocbibind}
% table of contents clickabel
\usepackage{hyperref}
% make file system neat and easier to manage
\usepackage{subfiles}
% packages
\usepackage{csquotes}
\usepackage{arydshln}
\usepackage{dsfont}
\usepackage{polynom}
\usepackage{empheq}
\usepackage{calc}  
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{systeme}
\usepackage{mathtools}
\usepackage{tikz}
\usepackage{amsfonts, mathrsfs, bm, amsmath, amssymb, bbm, amsthm}
\usepackage{ifthen}
\usepackage{enumerate}
% Make the preview part more eye friendly
\usepackage{xcolor}
\pagecolor[rgb]{1,1,1} % background
\color[rgb]{0,0,0} % foreground
% new command
\DeclareMathOperator*{\argmax}{arg~max}
\DeclareMathOperator*{\argmin}{arg~min}
% theorem block
\newtheorem{theorem}{Theorem}[section]
\newtheorem{note}{Note}
% cross page equation
\allowdisplaybreaks

\begin{document}
    \section{Information Theory}
        \subsection{Information}

            \begin{displayquote}
                \textit{Highly improbable events bring more information to us,
                while certain events bring no information.}
            \end{displayquote}

            The information of an event $ x $ will therefore depends on the
            probability distribution $ p(x) $ of its random variable $ X $.

        \subsection{Construct Information Fomula}

            Let $ h(\cdot)$ be a monotonic function of any distribution $ p(x)
            $ that returns the information of $ p(x) $. If $ x $ and $ y $ are
            unrelated events, we hope the information they take are also
            unrelated, so

            \begin{align}
                &h(x,y) = h(x) + h(y) \label{eq:1} \\
                &p(x,y) = p(x) p(y)   \label{eq:2}
            \end{align}

            Note that we can interpret $ h( p(x) ) $ as $ h(x) $ and
            interpret $ h( p(x, y) ) $ as $ h(x, y) $. 

            $ \log_{2}(x) $ is a monotonic function that satisfied both
            (\ref{eq:1}) and (\ref{eq:2}), hence we can define

            \begin{equation*}
                h(x) = - \log_2 p(x)
            \end{equation*}

            Then $ h(x) $ satisfies $ 2^{ h(x) } = \frac{1}{p(x)} $. We can
            interpret this as

            \begin{displayquote}
                $ h(x) $ \textit{is the amount of bits that being enough for
                representing} $ \frac{1}{p(x)} $ \textit{in binary}.
            \end{displayquote}

            When $ p(x) $ is low, the probability is low, we need more bits to
            represent it.

        \subsection{Entropy}

            Let $ X $ be the random variable of the state that transmitted from
            a sender to a receiver. Intuitively, \textit{the average amount of
            the information} that $ X $ carries is obtained by taking the
            expectation of information $ h(x) $ with respect to the p.d.f.
            $ p(x) $

            \begin{equation*}
                 \sum_{x \in X} p(x) h(x) = - \sum_{x \in X} p(x) \log_2 p(x)
            \end{equation*}

            This is called the \textit{entropy} of the random variable $ X $
            and denote it by $ \operatorname{H}(X) $ or $ \operatorname{H}(p) $
            or $ \operatorname{H}(x) $, based on the context of the paragraph.

            Since $ \lim_{p \rightarrow 0} p \ln p = 0 $,
            we just take $ p(x) \ln p(x) = 0 $ when we encounter $ p(x) = 0 $ for some
            $ x $.
            
            \subsubsection{Nats}

                In practice, we use $ \ln p(x)$ instead of $\log_2 p(x) $. That
                is,

                \begin{align*}
                    &h(x)                = - \ln p(x) \\
                    &\operatorname{H}(p) = - \sum_{x \in X} p(x) \ln p(x)
                \end{align*}

                In this situation, we said the information is measured in the
                units of 'nats'.

            \subsubsection{Entropy as Lower Bound}

                \begin{displayquote}
                    \textit{Entropy is a lower bound of the amount of bits that
                    a random variable can transmits.}
                \end{displayquote}

                by the \textit{Noiseless Coding Theorem}.

            \subsubsection{Noiseless Coding Theorem}

                \begin{displayquote}
                     $ N $ i.i.d. random variables each with entropy $
                     \operatorname{H}(X) $ can be compressed into more than $ N
                     \operatorname{H}(X) $ bits with negligible risk of
                     information loss, as $ N \rightarrow \infty $; but
                     conversely, if they are compressed into fewer than $ N
                     \operatorname{H}(X) $ bits it is virtually certain that
                     information will be lost.
                \end{displayquote}

                (ChatGPT) In essence, the theorem states that for any given
                data source with a certain probability distribution of symbols,
                it is possible to encode the source in such a way that the
                average length of the encoded message per symbol is close to
                the entropy of the source.

        \subsection{Maximize Entropy in Discrete Case}

            \subsubsection*{TL;DR}

                \begin{displayquote}
                    \textit{The distribution that can carry the most average
                    amount of information is the uniform.}
                \end{displayquote}

            \subsubsection*{Proof}
            
                Let $ X = \{x_i\}_{ i=1 }^M $ be a discrete random variable and
                let $ p $ be the distribution of $ X $. For the optimization
                problem

                \begin{equation*}
                     \max \left(
                        - \sum_{i=1}^M p(x_i) \ln p(x_i)
                    \right)
                \end{equation*}

                with the normalization constraint on the probabilities

                \begin{equation*}
                     \sum_{i=1}^M p(x_i) = 1
                \end{equation*}

                The Lagrangian is

                \begin{equation*}
                     \mathcal{L} = - \sum_{i=1}^M p(x_i) \ln p(x_i) +
                         \lambda \left(
                            \sum_{i=1}^M p(x_i) - 1
                         \right)
                \end{equation*}

                From $ \partial \mathcal{L} / \partial p(x_i) = - (\ln p(x_i) + 1) +
                \lambda = 0 $, we have $ \lambda = \ln p(x_i) + 1 $,
                $ \forall i = 1, \dots, M $. By $ \sum_{i=1}^M p(x_{i}) = 1 $ and $
                \lambda = \ln p(x_i) + 1 $, it's easy to get $ \lambda = \ln(1 / M)
                + 1 $, we then have 

                \begin{equation*}
                     p(x_i) = \frac{1}{M}, \forall i
                \end{equation*}

                is the stationary point.

                To verify the maximum, first compute the Hessian matrix

                \begin{equation*}
                     \frac{ \partial \mathcal{L} }{ \partial p(x_{i}) \partial
                     p(x_{j}) } = - I_{ij} \frac{1}{p(x_i)}
                \end{equation*}

                It's obvious that all the eigenvalues are negative (negative
                definite). So $ p(x_{i}) = \frac{1}{M} $ actually attains a
                maximum, and the maximum entropy is $ \operatorname{H}(p) = \ln
                M $.

        \subsection{Differential Entropy}

            Let $ X \subseteq \mathbb{R} $ be a continuous random variable and
            $ p(x) $ be the distribution of $ X $. By M.V.T, we know there
            exists some $ x_{i} $ such that

            \begin{equation*}
                 \int_{i \Delta}^{ (i+1) \Delta } p(x) dx = p(x_{i}) \Delta
            \end{equation*}

            where $ \Delta $ is the length of one partition of $ X $. Now for any
            $ x \in [i \Delta, (i + 1) \Delta] $, we can use $ p(x_i) \Delta $ to estimate
            its probability as long as $ \Delta $ small enough. Here comes an
            entropy estimation

            \begin{align*}
                \operatorname{H}_{\Delta}
                      &= - \sum_i p(x_i) \Delta \ln( p(x_i) \Delta) \\
                      &= - \sum_i p(x_i) \Delta \ln(p(x_i) \Delta) +
                             \sum_i p(x_i) \Delta \ln \Delta -
                                 \sum_i p(x_i) \Delta \ln \Delta    \\
                      &= - \sum_i p(x_i) \Delta \ln \left(
                                \frac{p(x_i) \Delta}{\Delta}
                            \right) - \left(
                                \sum_i p(x_i) \Delta
                            \right) \ln \Delta                      \\
                      &= - \sum_i p(x_i) \Delta \ln p(x_i) - \ln \Delta
            \end{align*}

            Note that $ \sum_i p(x_i) \Delta = \int_{x \in X} p(x) dx = 1 $.

            The limit of the first term of right hand side is

            \begin{equation*}
                 \lim_{\Delta \rightarrow 0} - \sum_i p(x_i) \Delta \ln p(x_i)
                 = - \int p(x) \ln p(x) dx
            \end{equation*}

            This integral is called the \textit{differential entropy}.

            The difference term $ \ln \Delta $ shows the fact that we need lots of bits to
            describe a continuous variable.

            The differential entropy can have negative values when $ \sigma^2 < 1 / (2 \pi e) $.

            For multi dimension random variable, the differential entropy is similar

            \begin{equation*}
                \operatorname{H}(p) = - \int p(\mathbf{x}) \ln p(\mathbf{x}) d \mathbf{x} 
            \end{equation*}

        \subsection{Conditional Entropy}

            Given a joint probability $ p(\mathbf{x}, \mathbf{y}) $. When $
            \mathbf{x} $ is known, the additional information needed to specify
            the corresponding value of $ \mathbf{y} $ is given by $ - \ln
            p(\mathbf{y} | \mathbf{x}) $. We can compute the
            \textit{conditional entropy}

            \begin{equation*}
                \operatorname{H}(\mathbf{y} | \mathbf{x}) = 
                 - \iint p(\mathbf{x}, \mathbf{y})
                    \ln p( \mathbf{y} | \mathbf{x} ) d \mathbf{y} d \mathbf{x}
            \end{equation*}

            Note that

            \begin{align*}
                \operatorname{H}(\mathbf{x}, \mathbf{y})
                      &= - \iint p(\mathbf{x}, \mathbf{y})
                         \ln p(\mathbf{x}, \mathbf{y}) d \mathbf{x} d \mathbf{y}                    \\
                      &= - \iint p(\mathbf{x}, \mathbf{y}) 
                         \ln ( p(\mathbf{y} | \mathbf{x}) p(\mathbf{x})) d \mathbf{x} d \mathbf{y}  \\
                      &= - \iint p(\mathbf{x}, \mathbf{y}) \ln p(\mathbf{y} | \mathbf{x})
                        d \mathbf{x} d \mathbf{y} -
                        \iint p(\mathbf{x}, \mathbf{y}) \ln p(\mathbf{x}) d \mathbf{x} d \mathbf{y} \\
                      &= - \iint p(\mathbf{x}, \mathbf{y}) \ln p(\mathbf{y} | \mathbf{x}) d \mathbf{x}
                          d \mathbf{y} - \int \left( \int p(\mathbf{x}, \mathbf{y}) d \mathbf{y} \right)
                          \ln p(\mathbf{x}) d \mathbf{x}                                            \\
                      &= \operatorname{H} (\mathbf{y} | \mathbf{x}) + \operatorname{H} (\mathbf{x})
            \end{align*}

        \subsection{Cross Entropy}

            The cross entropy of the distribution $ q(x) $ relative to a
            distribution $ p(x) $ is

            \begin{equation*}
                H(p,q) = -\mathrm{E}_p[\ln q] = - \sum_x p(x) \ln q(x) 
            \end{equation*}

            In deep learning, $ p(x) $ often refers to the ground truth label,
            and $ q(x) $ refers to the output from a deep neural network model.

            In information theory, minimize cross entropy means

            \begin{displayquote}
                 \textit{Minimizes the amount of information required to
                 specify the value of} $ x $ \textit{as a result of using} $
                 q(x) $.
            \end{displayquote}

        \subsection{Kullback-Leibler Divergence}

            Let $ p(x) $ be an unknown distribution and we use $ q(x) $ to
            approximate it. This will cause additional amount of information

            \begin{align*}
                \operatorname{KL}(p \| q)
                    &= \left(
                            - \int p(\mathbf{x}) \ln q(\mathbf{x}) d \mathbf{x}
                        \right) - \left(
                            - \int p(\mathbf{x}) \ln p(\mathbf{x}) d \mathbf{x}
                        \right)                          \\
                    &= -\int p(\mathbf{x}) \ln
                        \left\{
                            \frac{ q(\mathbf{x}) }{ p(\mathbf{x}) }
                        \right\} d \mathbf{x}
            \end{align*}

            This is known as the \textit{KL divergence} from $ q $ to $ p $.
            \textit{KL divergence} is also called the \textit{relative entropy}.

            Note that $ \operatorname{KL}(p \| q) \neq \operatorname{KL}(q \| p) $.

            \subsubsection*{TL;DR}

                $ \operatorname{KL}(p \| q) \geq 0 $, $ \operatorname{KL}(p \| q) = 0 \iff p = q $.


            \subsubsection*{Convex function}

                For $ 0 \leq \lambda \leq 1 $, a convex function satisfies

                \begin{equation*}
                    f(\lambda a + (1 - \lambda) b) \leq \lambda f(a) + (1 - \lambda) f(b)
                \end{equation*}

                A convex function is called strictly convex if the equality
                holds only when $ \lambda = 0 $ or $ \lambda = 1 $.

            \subsubsection*{Jensen's Inequality}

                Recall the \textit{Jensen's inequality}, for a convex function $ f $,

                \begin{equation*}
                    f \left( \sum_{i} \lambda_{i} x_{i}  \right) \leq \sum_{i} \lambda_{i} f(x_{i})
                \end{equation*}

                where $ \lambda_{i} \geq 0 $ and $ \sum_{i} \lambda_{i} = 1 $.

                When each $ \lambda_{i} $ becomes the probability $ p(x_{i}) $, we have

                \begin{equation*}
                    f\left( \operatorname{E}[x] \right) \leq \operatorname{E}[f(x)].
                \end{equation*}

                For continuous random variable, we have

                \begin{equation*}
                     f\left( \int \mathbf{x} p(\mathbf{x}) d \mathbf{x} \right)
                     \leq \int f(\mathbf{x}) p(\mathbf{x}) d \mathbf{x}
                \end{equation*}

            \subsubsection*{Proof}

                Observe that $ - \ln x $ is a strictly convex function, so by Jensen's inequality

                \begin{align*}
                    \operatorname{KL}(p \| q)
                        &= - \int p(\mathbf{x}) \ln
                             \frac{q(\mathbf{x})}{p(\mathbf{x})} d \mathbf{x} \\
                        &\geq - \ln \left(
                            \int p(\mathbf{x}) \frac{q(\mathbf{x})}{p(\mathbf{x})} d \mathbf{x}
                        \right)                                               \\
                        &=    - \ln \left( \int q(\mathbf{x}) d \mathbf{x} \right) = 0
                \end{align*}

                When the equality holds,

                \begin{align*}
                                & p(\mathbf{x}) \ln \frac{q(\mathbf{x})}{p(\mathbf{x})} = 0 \\
                    \Rightarrow & \ln \frac{q(\mathbf{x})}{p(\mathbf{x})} = 0               \\
                    \Rightarrow & \frac{q(\mathbf{x})}{p(\mathbf{x})} = 1                   \\
                    \Rightarrow & q(\mathbf{x}) = p(\mathbf{x})
                \end{align*}

        \subsection{Gaussian Maximizes Differential Entropy}

            We want to maximize

            \begin{equation*}
                 \operatorname{H}(p) = - \int p(x) \ln p(x) dx
            \end{equation*}

            with three natural constraints

            \begin{align*}
                &\int_{-\infty}^\infty p(x) dx = 1     \\
                &\int_{-\infty}^\infty x p(x) dx = \mu \\
                &\int_{-\infty}^\infty (x-\mu)^2 p(x) dx = \sigma^2
            \end{align*}

            The Lagrangian is

            \begin{multline*}
                 \mathcal{L}[p] = -\int_{\mathbb{R}} p(x) \ln p(x) dx +
                 \lambda_1 \left( \int_{\mathbb{R}} p(x) dx - 1 \right) + \\
                 \lambda_2 \left( \int_{\mathbb{R}} x p(x) dx - \mu \right) +
                 \lambda_3 \left( \int_{\mathbb{R}} (x - \mu)^2 p(x) dx - \sigma^2 \right)
            \end{multline*}

            This is a functional. The derivative of a functional is
            denoted by $ \frac{\delta \mathcal{L}}{\delta p} $ and is defined to satisfy

            \begin{equation*}
                 \int \frac{\delta \mathcal{L}[p]}{\delta p} \phi(x) dx =
                 \lim_{\epsilon \rightarrow 0}\frac{\mathcal{L}[p + \epsilon
                 \phi] - \mathcal{L}[p]}{\epsilon} = 
                 \left[
                     \frac{d}{d \epsilon} \mathcal{L}[p + \epsilon \phi]
                 \right]_{\epsilon = 0}
            \end{equation*}

            where $ \phi(x) $ is a variation term.

            Deriving from the definition

            \begin{equation*}
                 \int \frac{\delta \mathcal{L}[p]}{\delta p} \phi(x) dx = \int 
                 \left( - ( \ln p(x) + 1 ) + \lambda_1 + \lambda_2 x +
                 \lambda_3 (x - \mu)^2 \right) \phi(x) dx
            \end{equation*}

            We have the actual form of $ \frac{\delta \mathcal{L}[p]}{\delta p} $ and can let it be zero then get

            \begin{equation*}
                 p(x) = e^{ -1 + \lambda_1 + \lambda_2 x + \lambda(x - \mu)^2 }
            \end{equation*}

            Substitute this result back to three constraints leading to

            \begin{equation*}
                 p(x) = \frac{1}{ (2 \pi \sigma^2)^{\frac{1}{2}} } e^{ -\frac{(x - \mu)^2}{2 \sigma^2} }
            \end{equation*}

            which is the Gaussian.

            To verify the maximum, let $ f(x) $ be any distribution has the
            variance $ \sigma^2 $. Since differential entropy is translation invariant, we can also assume
            $ f(x) $ has the same mean $ \mu $. Now consider the KL divergence

            \begin{align*}
                & \operatorname{KL}(f \| p) = -\int f(x) \ln \left( \frac{p(x)}{f(x)} \right) dx \\
                =& -\operatorname{H}(f) - \int f(x) \ln \left(
                         \frac{1}{ (2 \pi \sigma^2)^{\frac{1}{2}} }
                         e^{ -\frac{(x - \mu)^2}{2 \sigma^2} } 
                    \right) dx                                                                   \\
                =& -\operatorname{H}(f) + \frac{1}{2} \ln (2 \pi \sigma^{2}) +
                    \frac{1}{2 \sigma^{2}} \int f(x) (x - \mu)^{2} dx                            \\
                =& -\operatorname{H}(f) + \frac{1}{2} \ln (2 \pi \sigma^{2}) +
                    \frac{\sigma^{2}}{2 \sigma^{2}}                                              \\
                =& -\operatorname{H}(f) + \frac{1}{2} \ln (2 \pi \sigma^{2}) + \frac{1}{2}       \\
                =& -\operatorname{H}(f) + \operatorname{H}(p) \geq 0
            \end{align*}

            Hence

            \begin{equation*}
                 H(p) \geq H(f), \forall f
            \end{equation*}

            The corresponding maximum entropy is,

            \begin{equation*}
                 \operatorname{H}(p) = \frac{1}{2} (1 + \ln(2 \pi \sigma^2))
            \end{equation*}

        \subsection{KL Divergence in Deep Learning}

            Let $ p(\mathbf{x}) $ be an unknown target distribution. We have $
            N $ training data $ \{ x_{i} \}_{i=1}^{N} $ drawn from it. Let $
            q(\mathbf{x} | \theta) $ be a neural network tries to approximate $
            p(\mathbf{x}) $ with the model weight $ \theta $. In theory, the 
            training process should minimize the KL divergence

            \begin{equation*}
                 \operatorname{KL}(p \| q) = -\operatorname{E}_{p} \left[ \ln
                 \frac{q}{p} \right] = - \int p(\mathbf{x}) \ln
                 \frac{q(\mathbf{x} | \theta)}{p(\mathbf{x})} d \mathbf{x}.
            \end{equation*}

            Because $ p(\mathbf{x}) $ is unknown, we can't directly compute the
            KL divergence. However, we can use sample mean to estimate it

            \begin{equation*}
                 \operatorname{KL}(p \| q) = -\operatorname{E}_{p} \left[ \ln
                 \frac{q}{p} \right] \approx -\frac{1}{N} \sum_{i=1}^{N} \ln
                 \left( \frac{q(\mathbf{x}_{i} | \theta)}{p(\mathbf{x})} \right)
            \end{equation*}

            During training, $ N $ is fixed, so

            \begin{equation*}
                 \operatorname{KL}(p \| q) \approx \sum_{i=1}^{N} -\ln
                 q(\mathbf{x}_{i} | \theta) + \ln p(\mathbf{x}_{i}).
            \end{equation*}

            $ \ln p(\mathbf{x}) $ is not related to the training, so minimizing
            $ \sum_{i} -\ln q(\mathbf{x}_{i} | \theta) $ is equivalent to
            minimizing the KL divergence.

        \subsection{Mutual Information}

           The following KL divergence is called the \textit{Mutual
           Information} $ \operatorname{I}[\mathbf{x}, \mathbf{y}] $,

           \begin{equation*}
               \operatorname{I}[\mathbf{x}, \mathbf{y}] =
               \operatorname{KL}(p(\mathbf{x}, \mathbf{y}) \| p(\mathbf{x})
               p(\mathbf{y})) = - \iint p(\mathbf{x} \mathbf{y}) \ln  \left(
                   \frac{p(\mathbf{x}) p(\mathbf{y})}{p(\mathbf{x}, \mathbf{y})}
               \right) d \mathbf{x}
           \end{equation*}
\end{document}
