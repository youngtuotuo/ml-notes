\documentclass{article}
\usepackage[a4paper]{geometry}
\usepackage[utf8]{inputenc} 
\usepackage{indentfirst}
%list code
\usepackage{listings}
%content include references
\usepackage[nottoc,numbib]{tocbibind}
% table of contents clickabel
\usepackage{hyperref}
% make file system neat and easier to manage
\usepackage{subfiles}
% packages
\usepackage{csquotes}
\usepackage{arydshln}
\usepackage{dsfont}
\usepackage{polynom}
\usepackage{empheq}
\usepackage{calc}  
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{systeme}
\usepackage{mathtools}
\usepackage{tikz}
\usepackage{amsfonts, mathrsfs, bm, amsmath, amssymb, bbm, amsthm}
\usepackage{ifthen}
\usepackage{enumerate}
% Make the preview part more eye friendly
\usepackage{xcolor}
\pagecolor[rgb]{1,1,1} % background
\color[rgb]{0,0,0} % foreground
% new command
\DeclareMathOperator*{\argmax}{arg~max}
\DeclareMathOperator*{\argmin}{arg~min}
% theorem block
\newtheorem{theorem}{Theorem}[section]
\newtheorem{note}{Note}
% cross page equation
\allowdisplaybreaks

\begin{document}
    \section{Support Vector Machine}
        Suppose there are $N$ data points $\mathbf{x}_1,\dots,\mathbf{x}_N$ and let $t_1,\dots,t_N$ be their labels where $t_k\in\{-1,1\}$ for all $k$. We want to find suitable $\phi(\mathbf{x}_k)$, weight $\mathbf{w}$ and bias $b$ that satisfies
        \[
            \mathbf{w}^T \phi(\mathbf{x}_k) + b 
                    = \left\{
                    \begin{array}{rl}
                        > 0 & \mathrm{if ~} t_k = 1\\[0.2cm]
                        < 0 & \mathrm{if ~} t_k = -1
                    \end{array}
                      \right. \quad \mathrm{for~all~}k
        \]
        This is a linear classifier in feature space. The distance of each data point to this hyper plane is
        \[
            \frac { |\mathbf{w}^T \phi(\mathbf{x}_k) + b| } {\|\mathbf{w}\|}
        \]
        According to \textit{statistical learning theory}(not actually understand), we want to maximize the minimal distance
        \[
            \argmax_{\mathbf{w}, b}\left\{\frac{1}{\|\mathbf{w}\|} \min_{k} |\mathbf{w}^T \phi(\mathbf{x}_k) + b|\right\}
        \]
        Since $t_k(\mathbf{w}^T \phi(\mathbf{x}_k) + b) > 0$ and $t_k\in\{-1,1\}$, the problem is equivalent to
        \[
            \argmax_{\mathbf{w},b}\left\{\frac{1}{\|\mathbf{w}\|}\min_k t_k(\mathbf{w}^T \phi(\mathbf{x}_k) + b)\right\}
        \]
        Observe that any rescaling of $\mathbf{w}$ and $b$ won't change the ultimate value, hence we can set
        \[
            t_k (\mathbf{w}^T \phi(\mathbf{x}_k) + b) = 1
        \]
        for those $\mathbf{x}_k$ that are closest to the hyper plane. Then other points yield
        \[
            t_k (\mathbf{w}^T \phi(\mathbf{x}_k) + b) \geq 1
        \]
        Now we simplify the problem into
        \[
            \argmax_{\mathbf{w},b}\frac{1}{ \|\mathbf{w}\| } \mathrm{\quad s.t. \quad} t_k (\mathbf{w}^T \phi(\mathbf{x}_k) + b) \geq 1 \mathrm{~for~all~}k
        \]
        It's equivalent to
        \[
            \argmin_{\mathbf{w}, b}\frac{1}{2} \|\mathbf{w}\|^2 \mathrm{\quad s.t. \quad} t_k(\mathbf{w}^T \phi(\mathbf{x}_k) + b)\geq 1\mathrm{~for~all~}k
        \]
        And gives the Lagrangian function
        \[
            \mathcal{L}(\mathbf{w}, b, \mathbf{a}) = \frac{1}{2} \|\mathbf{w}\|^2 + \sum_{k=1}^N a_k \big[1 - t_k (\mathbf{w}^T \phi(\mathbf{x}_k) + b)\big]
        \]
        where $\mathbf{a}=(a_1,\dots,a_N)^T\geq \mathbf{0}$. For
        \[
            \max_{\mathbf{a}}\min_{\mathbf{w},b}\mathcal{L}(\mathbf{w},b,\mathbf{a})\mathrm{\quad s.t. \quad}\mathbf{a}\geq 0
        \]
        let gradient vanish with respect to $\mathbf{w}$ and $b$
        \begin{align*}
            &\nabla_{\mathbf{w}}\mathcal{L}(\mathbf{w},b,\mathbf{a}) = \mathbf{w}-\sum_{k=1}^N a_k t_k \phi(\mathbf{x}_k) = 0\\
            &\nabla_b \mathcal{L}(\mathbf{w},b,\mathbf{a}) = \sum_{k=1}^N a_kt_k = 0
        \end{align*}
        Substitute back then yield the dual form
        \[
            \max_{\mathbf{a}}\sum_{k=1}^N a_k - \frac{1}{2} \sum_{k=1}^N \sum_{m=1}^N a_k a_m t_k t_m \phi(\mathbf{x}_k)^T \phi(\mathbf{x}_m)
        \]
        subject to
        \begin{align*}
            a_k \geq 0,&\quad k=1,\dots,N\\
            \sum_{k=1}^N a_k t_k = 0.&
        \end{align*}
        
        Let $k(\mathbf{x},\mathbf{x}')=\phi(\mathbf{x})^T \phi(\mathbf{x}')$ stands for the kernel function and let $y(\mathbf{x}) = \mathbf{w}^T \phi(\mathbf{x}) + b$. When $\mathbf{w}$ is the solution, $\mathbf{w} = \sum_k a_k t_k \phi(\mathbf{x}_k)$. Put this into $y(\mathbf{x})$ and have
        \[
            y(\mathbf{x}) = \sum_{k=1}^N a_k t_k k(\mathbf{x}_k, \mathbf{x}) + b
        \]
        here we express the classifier in terms of $\{a_k\}$ and the kernel function $k(\mathbf{x}, \mathbf{x}')$.
        
        \subsection{Apply KKT Condition}
            We have a primal problem
            \[
                \argmin_{\mathbf{w},b} \frac{1}{2} \|\mathbf{w}\|^2 \mathrm{\quad s.t. \quad} t_k y(\mathbf{x}_k) -1 \geq 0 \mathrm{~for~all~}k
            \]
            and a dual problem
            \[
                \max_{\mathbf{a}}\sum_{k=1}^N a_k - \frac{1}{2} \sum_{k=1}^N \sum_{m=1}^N a_k a_m t_k t_m k(\mathbf{x}_k, \mathbf{x}_m) \mathrm{\quad s.t. \quad} a_k \geq 0,~\forall~k\mathrm{~and~}\sum_{k=1}^N a_k t_k = 0
            \]
            The KKT condition needs further constraint that
            \[
                a_k \{t_k y(\mathbf{x}_k) - 1\} = 0
            \]
            This implies $a_k=0$ or $t_k y(\mathbf{x}_k)=1$. Any data point has $a_k=0$ will not contribute to the classifier. The rest data points have $t_k y(\mathbf{x}_k)=1$ are called \textit{support vector}.This tells us that we only need support vectors to predict new point though we need whole data to train. Mathematically, choose one support vector $\mathbf{x}'$, we can get $b$ by
            \[
                t' y(\mathbf{x}') = t'\left\{ \sum_{m} a_m t_m k(\mathbf{x}_m, \mathbf{x}') + b \right\} = 1
            \]
            where $m$ stands for the index of support vector. In practical, multiply each side by one label $t_k$ of one  support vector and have
            \[
                b = t_k - \left\{ \sum_m a_m t_m k(\mathbf{x}_m, \mathbf{x}_k) \right\} \mathrm{~for~all~}k
            \]
            Take the average of all possible $b$ as the final one
            \[
                b = \frac{1}{M} \sum_{k}\left(t_k - \left\{ \sum_m a_m t_m k(\mathbf{x}_m, \mathbf{x}_k) \right\}\right)
            \]
            where $M$ is the number of support vectors and $k$ and $m$ are both the index of support vector.

    \section{Soft Margin Support Vector Machine}
        The original SVM is
        \[
            \min_{\mathbf{w},b} \frac{1}{2} \|\mathbf{w}\|^2 \mathrm{\quad s.t. \quad} t_k y(\mathbf{x}_k) \geq 1 \mathrm{~for~all~}k
        \]
        To allow some points can be misclassified, we introduce each point a \textit{slack variable} $\xi_k$ that is defined by
        \[
            \xi_k = \left\{
                        \begin{array}{rl}
                            0 & \mathrm{if ~} t_k y(\mathbf{x}_k) \geq 1 \\
                            |t_k - y(\mathbf{x}_k)| & \mathrm{otherwise}
                        \end{array}
                    \right.    
        \]
        Look deeper into this definition. If $0 < t_k y(\mathbf{x}_k) < 1$, we have $0 < y(\mathbf{x}_k) < 1$ or $-1 < y(\mathbf{x}_k) < 0$. Hence
        $0 < \xi_k = 1 - t_k y(\mathbf{x}_k) < 1$. If $t_k y(\mathbf{x}_k) = 0$, $\xi_k = 1 - t_k y(\mathbf{x}_k) = 1$. If $t_k y(\mathbf{x}_k) < 0$, $\xi_k = 1 - t_k y(\mathbf{x}_k) > 1$. In summary
        \[
            \xi_k \left\{
                        \begin{array}{ll}
                            = 0 & \mathrm{if ~} t_k y(\mathbf{x}_k) \in [1,\infty) \\
                            = 1 - t_k y(\mathbf{x}_k) & 
                            \left\{
                                \begin{array}{ll}
                                    \in (0, 1) & \mathrm{if ~} t_k y(\mathbf{x}_k) \in (0, 1)\\
                                    = 1 & \mathrm{if ~} t_k y(\mathbf{x}_k) = 0\\
                                    > 1 & \mathrm{if ~} t_k y(\mathbf{x}_k) \in (-\infty, 0)
                                \end{array}
                            \right.
                        \end{array}
                    \right.
        \]
        
        Replace the constrain with
        \[
            t_k y(\mathbf{x}_k) \geq 1 - \xi_k \mathrm{~for~all~}k
        \]
        This is so called \textit{soft margin}.

        When there exist outliers, they'll have extremely large $\xi_k$. To avoid this, here comes the soft SVM that also minimize slack vairable
        \[
            \min_{\mathbf{w},b, \mathbf{\xi}} C \sum_{k=1}^N \xi_k + \frac{1}{2} \|\mathbf{w}\|^2 \mathrm{\quad s.t. \quad} t_k y(\mathbf{x}_k) \geq 1 - \xi_k \mathrm{~for~all~}k
        \]
        where $C$ is some constant. Briefly speaking, slack vairables relax some points' constarint, their $t_k y(\mathbf{x}_k)$ only needs to larger than some value smaller than $1$. That's why we call this \textit{soft margin}.

        \subsection{Apply KKT Condition}
            The Lagrangian of soft SVM is
            \[
                \mathcal{L}(\mathbf{w},b,\xi,\mathbf{a},\mathbf{\mu}) = \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{k=1}^N \xi_k - \sum_{k=1}^N a_k \{ t_k y(\mathbf{x}_k) - 1 + \xi_k\} - \sum_{k=1}^N \mu_k \xi_k
            \]
            where $a_k,\mu_k \geq 0$ , $t_k y(\mathbf{x}_k) - 1 + \xi_k \geq 0$ and note that slack variables are non negative $\xi_k \geq 0$. The KKT conditions are
            \[
                \begin{array}{rll}
                    \mathbf{Dual~Feasibility~} & a_k \geq 0, & \mu_k \geq 0 \\
                    \mathbf{Primal~Feasibility~} & t_k y(\mathbf{x}_k) - 1 + \xi_k \geq 0, & \xi_k \geq 0 \\
                    \mathbf{Complmentary~Slackness~} & a_k (t_k y(\mathbf{x}_k) - 1 + \xi_k) = 0, & \mu_k \xi_k = 0
                \end{array}
            \]
            Use $y(\mathbf{x}) = \mathbf{w}^T \phi(\mathbf{x}) + b$ and compute  gradients of $\mathcal{L}$ with respect to $\mathbf{w}$, $b$ and $\xi$
            \begin{align*}
                & \nabla_{\mathbf{w}} \mathcal{L} = \mathbf{w} - \sum_{k=1}^N a_k t_k \phi(\mathbf{x}_n) = 0 \\
                & \nabla_b \mathcal{L} = \sum_{k=1}^N a_k t_k = 0 \\
                & \nabla_{\xi_k} \mathcal{L} = a_k - C - \mu_k = 0
            \end{align*}
            Substitute back and get the dual form
            \[
                \max_{\mathbf{a}} \sum_{k=1}^N a_k - \frac{1}{2} \sum_{k=1}^N \sum_{m=1}^N a_k a_m t_k t_m k(x_k, x_m) \mathrm{\quad s.t. \quad} 0 \leq a_k \leq C,~ \forall~k \mathrm{~and~} \sum_{k=1}^N a_k t_k = 0
            \]
            It's the same as normal SVM, the only difference is the constraint of $a_k$, which is known as the \textit{box constraint}.
\end{document}
