\documentclass{article}
\usepackage[a4paper]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{indentfirst}
%list code
\usepackage{listings}
%content include references
\usepackage[nottoc,numbib]{tocbibind}
% table of contents clickabel
\usepackage{hyperref}
% make file system neat and easier to manage
\usepackage{subfiles}
% packages
\usepackage{csquotes}
\usepackage{arydshln}
\usepackage{dsfont}
\usepackage{polynom}
\usepackage{empheq}
\usepackage{calc}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{systeme}
\usepackage{mathtools}
\usepackage{tikz}
\usepackage{amsfonts, mathrsfs, bm, amsmath, amssymb, bbm, amsthm}
\usepackage{ifthen}
\usepackage{enumerate}
% Make the preview part more eye friendly
\usepackage{xcolor}
\pagecolor[rgb]{1,1,1} % background
\color[rgb]{0,0,0} % foreground
% new command
\DeclareMathOperator*{\argmax}{arg max}
\DeclareMathOperator*{\argmin}{arg min}
% theorem block
\newtheorem{theorem}{Theorem}[section]
\newtheorem{note}{Note}
% cross page equation
\allowdisplaybreaks

\begin{document}

    \section*{Batch Normalization, \href{https://arxiv.org/abs/1502.03167}{https://arxiv.org/abs/1502.03167}}

        \subsection*{Target}
            
            Reduce the \textit{internal covariate shift}.

            \textit{Covariate shift}: The input distribution to a learning
            system changes.

            \textit{Internal covariate shift}: In the course of
            training, the change in the distributions of internal nodes of a
            deep network.

        \subsection*{Training Time}

            Let $ \{ f_{1}, f_{2}, f_{3}, \dots, f_{n} \} $ be the batched
            feature vector of some layer (a.k.a. that layer's output, either
            pre-activation or post-activation). Let's say each $ f_{i} \in
            \mathbb{R}^d $. The Batch Normalization first computes

            \begin{align*}
                & \mu = \frac{1}{n} \sum_{i = 1}^n f_{i} \\
                & \sigma = \sqrt{\frac{\sum_{i = 1}^n ( f_{i} - \mu )^2}{n}} \\
                & \tilde{f}_{i} = \frac{ f_{i} - \mu }{\sigma},~\mu, \sigma \in \mathbb{R}^d
            \end{align*}

            second, computes the affine transform

            \begin{align*}
                 \hat{f}_{i} = \gamma \cdot \tilde{f}_{i} + \beta
            \end{align*}

            where all arithmetic are element-wise, and $ \gamma, \beta \in \mathbb{R} $ are learnable. 

            We'll use each $ \hat{f}_{i} $ as next layer's input. Each new
            batch features $ \{ \hat{f}_{i} \}_{i = 1}^n $ has mean $ \mathbf{0} \in \mathbb{R} $ and
            standard deviation $ \mathbf{1} \in \mathbb{R} $.

            Note that the batch normalization is computed independently for
            each channel in CNN.

        \subsection*{Testing Time}

            During the testing time, mostly we don't have a bachted data. Hence
            no on-line $ \mu, \sigma $ to compute. Instead, we use the training
            data to help us get them.

            For each $ i $-th batch, we have $ \mu_{i}, \sigma_{i} $. We can then use the
            Exponential Moving Average to compute the $ \bar{\mu}, \bar{\sigma} $ for testing

            \begin{align*}
                & \bar{\mu} = \alpha \bar{\mu} + ( 1 - \alpha ) \mu_{i} \\
                & \bar{\sigma} = \alpha \bar{\sigma} + ( 1 - \alpha ) \sigma_{i}
            \end{align*}

            where $ \alpha \in [0, 1] $ is a pre-defined hyper parameter and $ \bar{\mu},
            \bar{\sigma} $ are both initialized to $ \mathbf{0} $. In pytorch,
            $ \alpha $ is called \textit{momentum} and the default value is $
            0.1 $.

            We use

            \begin{equation*}
                 \hat{f} = \frac{f - \bar{\mu}}{\bar{\sigma}} \cdot \gamma + \beta
            \end{equation*}

            as the testing time batch normalization.

    \section*{Layer Normalization, \href{https://arxiv.org/abs/1607.06450}{https://arxiv.org/abs/1607.06450}}
        

    \section*{Instance Normalization, \href{https://arxiv.org/abs/1607.08022}{https://arxiv.org/abs/1607.08022}}
        
        
    \section*{Group Normalization, \href{https://arxiv.org/abs/1803.08494}{https://arxiv.org/abs/1803.08494}}
        

    \section*{Weight Normalization, \href{https://arxiv.org/abs/1602.07868}{https://arxiv.org/abs/1602.07868}}
        

    \section*{Weight Normalization, \href{https://arxiv.org/abs/1705.10941}{https://arxiv.org/abs/1705.10941}}
        

\end{document}
